{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Content Audit Script [Keep / Refresh Mapping].ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7ByFc_3ZKz3"
      },
      "source": [
        "# Content Audit [Keep / Refresh / Redirect Mapping]\n",
        "\n",
        "[INSERT VIDEO WALKTHROUGH]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjAsXNbzn54o",
        "outputId": "8a3172bd-4804-47a9-feb0-6bc0b71e6498"
      },
      "source": [
        "#@title Mount Drive to the Notebook { display-mode: \"form\" }\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bf27EDik1oJJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d48f5740-1058-4e23-82fc-f4de4688debe"
      },
      "source": [
        "#@title Import API Module from /My Drive/Colab Notebooks/ { display-mode: \"form\" }\n",
        "import sys\n",
        "sys.path.insert(0, \"drive/My Drive/Colab Notebooks/\")\n",
        "# sys.path.insert(0, \"drive/My Drive/GSC_custom_api_library\")\n",
        "from api import *\n",
        "colab_path = \"/content/drive/My Drive/Colab Notebooks/\" \n",
        "# colab_path = \"/content/drive/My Drive/GSC_custom_api_library\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to the following link in your browser: https://accounts.google.com/o/oauth2/v2/auth?client_id=733455143894-uob3vcm050frdb8c0fd4qjetg2lrv9tr.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fwebmasters.readonly+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fanalytics.readonly&access_type=offline&response_type=code\n",
            "Enter verification code: 4/1AY0e-g5qErFoFo4tu173lA2yFAoU_OAWUlgIU-gFbM6nrqqxMbvtm72csac\n",
            "Credentials saved for later.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUukgyBKn54r"
      },
      "source": [
        "#@title Import External Dependencies { display-mode: \"form\" }\n",
        "import pandas\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from bs4 import BeautifulSoup, NavigableString\n",
        "import requests\n",
        "import random\n",
        "import lxml\n",
        "import lxml.html\n",
        "import lxml.etree\n",
        "from lxml.etree import ParseError\n",
        "from lxml.etree import ParserError\n",
        "from urllib.parse import urlparse\n",
        "import os\n",
        "from google.colab import data_table\n",
        "from IPython.display import HTML"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0ht4o9l_MBW"
      },
      "source": [
        "<br>\n",
        "\n",
        "# Input Variables [for Extraction + Variables] \n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7Wg9TxhJm_s",
        "cellView": "form"
      },
      "source": [
        "#@markdown #  Set `xpath_selector` to scrape MAIN CONTENT ONLY { run: \"auto\", display-mode: \"form\" }\n",
        "#@markdown *Almost all sites have multiple templates - combine xpaths for each template using \"|\").    \n",
        "#@markdown E.g., `//div[@class='example-1']|//div[@class='example-2']|//div[@class='example-3']`\n",
        "\n",
        "xpath_selector = \"//div[@class='entry-content']|//div[@class='body']\" #@param {type: \"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown # Search Console Click KPIs - range for quality / value considerations \n",
        "#@markdown Low range [not much traffic]  \n",
        "low_clicks_1 = 1  #@param {type: \"integer\"}\n",
        "low_clicks_2 = 99  #@param {type: \"integer\"}\n",
        "#@markdown Strong range [strong traffic]  \n",
        "strong_clicks_1 = 100  #@param {type: \"integer\"}\n",
        "strong_clicks_2 = 1000000  #@param {type: \"integer\"}\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown # Word Count of Main Content - range for quality / value considerations \n",
        "#@markdown Thin range [not much traffic]  \n",
        "thin_words_1 = 0  #@param {type: \"integer\"}\n",
        "thin_words_2 = 100  #@param {type: \"integer\"}\n",
        "#@markdown Okay range [some traffic]  \n",
        "okay_words_1 = 101  #@param {type: \"integer\"}\n",
        "okay_words_2 = 999  #@param {type: \"integer\"}\n",
        "#@markdown Strong range [strong traffic]  \n",
        "strong_words_1 = 1000  #@param {type: \"integer\"}\n",
        "strong_words_2 = 100000  #@param {type: \"integer\"}"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFXRRpAIXYdX"
      },
      "source": [
        "<br>\n",
        "\n",
        "# Input Variables [for Data Pull] \n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQqlx-IWXfLb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "d289574d-4dac-4328-ca78-0d8d487924de"
      },
      "source": [
        "\n",
        "#@markdown ### Set `domain_lookup` to match the homepage URL\n",
        "domain_lookup = 'https://mint.intuit.com/'  #@param {type: \"string\"}\n",
        "#@markdown ---\n",
        "#@markdown ### Set `gsc_property` to match the GSC UI property name \n",
        "#@markdown (*For <b>\"domain properties\"</b> use format \"sc-domain:domain.com\")  \n",
        "gsc_property = 'https://mint.intuit.com/'  #@param {type: \"string\"}\n",
        "#@markdown ---\n",
        "#@markdown ### Set `startdate` and `enddate` to desired analysis range \n",
        "#@markdown (*Recommend no longer than 3 months from present due to shifting query rankings)  \n",
        "startdate = '2021-01-01'  #@param {type: \"date\"}\n",
        "enddate = '2021-04-30'  #@param {type: \"date\"}\n",
        "#@markdown ---\n",
        "#@markdown ### Set page <b>INCLUSIONS</b> with format *urlID1|urlID2|...*\n",
        "#@markdown (*Leave blank if no exclusions needed)\n",
        "page_inclusions = \"blog\" #@param {type: \"string\"}\n",
        "#@markdown ---\n",
        "\n",
        "import os\n",
        "\n",
        "domain_name = domain_lookup.split(\"www.\")[-1].split(\"//\")[-1].split(\".\")[0]\n",
        "date = datetime.today().strftime('%Y-%m-%d')\n",
        "\n",
        "domain_clean = urlparse(\"{}\".format(domain_lookup)).netloc\n",
        "path_step1 = \"{}/{}/\".format(colab_path,domain_clean)\n",
        "path = \"{}/{}/Content_Audit_{}\".format(colab_path,domain_clean,date)\n",
        "path_rawData = \"{}/{}/Raw Data (Archive)\".format(colab_path,domain_clean,date)\n",
        "date = datetime.today().strftime('%Y-%m-%d')\n",
        "\n",
        "try:\n",
        "    os.mkdir(path_step1)\n",
        "except OSError:\n",
        "    print (\"Creation of the directory %s failed\" % path_step1)\n",
        "else:\n",
        "    print (\"Successfully created the directory %s \" % path_step1)\n",
        "\n",
        "try:\n",
        "    os.mkdir(path_rawData)\n",
        "except OSError:\n",
        "    print (\"Creation of the directory %s failed\" % path_rawData)\n",
        "else:\n",
        "    print (\"Successfully created the directory %s \" % path_rawData)\n",
        "\n",
        "try:\n",
        "    os.mkdir(path)\n",
        "except OSError:\n",
        "    print (\"Creation of the directory %s failed\" % path)\n",
        "else:    print (\"Successfully created the directory %s \" % path)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creation of the directory /content/drive/My Drive/Colab Notebooks//mint.intuit.com/ failed\n",
            "Creation of the directory /content/drive/My Drive/Colab Notebooks//mint.intuit.com/Raw Data (Archive) failed\n",
            "Creation of the directory /content/drive/My Drive/Colab Notebooks//mint.intuit.com/Content_Audit_2021-05-31 failed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CMo2Qvf9s4Q"
      },
      "source": [
        "# Automated Script - No Other Manual Changes Are Needed\n",
        "### Open tab to see additional details"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kY5Zws4EV6o"
      },
      "source": [
        "<h2>PART 1: Get GSC Data </h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRzNdMFvn54u",
        "scrolled": true,
        "outputId": "1004297b-c0e1-4fea-be31-f963e532a5c2"
      },
      "source": [
        "#@title Collect data from Google Search Console { display-mode: \"form\" }\n",
        "domain_name = domain_lookup.split(\"www.\")[-1].split(\"//\")[-1].split(\".\")[0]\n",
        "\n",
        "# Options: 'date,' 'device,' 'page,' , 'query' and \"country\"\n",
        "dimensions=['page']\n",
        "\n",
        "gsc_df = gscservice.get_site_data(\n",
        "    gsc_property,\n",
        "    startdate=startdate,\n",
        "    enddate=enddate,\n",
        "    dimensions=dimensions,\n",
        "    output_fn=\"{}/{}/Raw Data (Archive)/rawData_{}_{}_{}_by_{}.csv\".format(colab_path,domain_clean,domain_name, startdate.replace(\"-\",\"\"), enddate.replace(\"-\",\"\"), '_'.join(dimensions))\n",
        ")\n",
        "\n",
        "# Filter to only non-brand && specified page type (if any)\n",
        "# kw_filter = ~gsc_df[\"query\"].str.contains(\"{}\".format(brand_exclusions), case = False, regex=True)\n",
        "# gsc_df = gsc_df[kw_filter]\n",
        "filter_pageType = gsc_df[\"page\"].str.contains(\"{}\".format(page_inclusions), case = False, regex=True)\n",
        "gsc_df = gsc_df[filter_pageType]\n",
        "\n",
        "gsc_df_clean = (\n",
        "    gsc_df.groupby([\"page\"])\n",
        "    .agg({\"clicks\": \"sum\"})\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "datasetID = \"{}_{}_{}_by_{}\".format(domain_name, startdate.replace(\"-\",\"\"), enddate.replace(\"-\",\"\"), '_'.join(dimensions))\n",
        "gsc_df_clean.insert(loc = 0, column = \"gsc_datasetID\", value = datasetID)\n",
        "gsc_df_clean.insert(loc = 1, column = \"domain\", value = domain_lookup)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reloading Existing: /content/drive/My Drive/Colab Notebooks//mint.intuit.com/Raw Data (Archive)/rawData_mint_20210101_20210430_by_page.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkZACtXpgKUn"
      },
      "source": [
        "#@title Calculate Clicks Range and Value { display-mode: \"form\" }\n",
        "conds = [\n",
        "    gsc_df_clean.clicks == 0,\n",
        "    gsc_df_clean.clicks.between(int(low_clicks_1),int(low_clicks_2)),\n",
        "    gsc_df_clean.clicks.between(int(strong_clicks_1),int(strong_clicks_2))\n",
        "]\n",
        "\n",
        "clicks_range = [\n",
        "    '0',\n",
        "    f'{low_clicks_1} to {low_clicks_2}', \n",
        "    f'{strong_clicks_1}+'\n",
        "]\n",
        "clicks_value = [\n",
        "    'None',\n",
        "    'Low',\n",
        "    'Strong'\n",
        "]\n",
        "\n",
        "gsc_df_clean['clicks_range'] = np.select(conds,clicks_range)\n",
        "gsc_df_clean['clicks_value'] = np.select(conds,clicks_value)\n",
        "\n",
        "# del gsc_df['index']\n",
        "\n",
        "excel_file = pandas.ExcelWriter(os.path.join(path,\"final-content-audit_{}-{}.xlsx\".format(domain_name,date)))\n",
        "\n",
        "# Preview the export from GSC API\n",
        "# gsc_df.to_csv(os.path.join(path,\"step1_raw-page-query-data_{}-{}.csv\".format(domain_name,date)),  index=False)\n",
        "# gsc_df.to_excel(\n",
        "#     excel_file,\n",
        "#     sheet_name='GSC',\n",
        "#     header=True)\n",
        "# data_table.DataTable(gsc_df, include_index=False, num_rows_per_page=10)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sB_wU_tzQno"
      },
      "source": [
        "#@title DEV - keep top 10 pages by clicks { display-mode: \"form\" }\n",
        "# gsc_df_clean = gsc_df_clean.sort_values(by=\"clicks\", ascending=False).reset_index().head(10)\n",
        "# gsc_df_clean"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPUKXuaCn541",
        "scrolled": true
      },
      "source": [
        "#@title Scrape Pages & Calculate Words Range and Value { display-mode: \"form\" }\n",
        "### [BELOW AGGREGATES PAGES FOR SCRAPER]\n",
        "\n",
        "def GET_UA():\n",
        "  uastrings = [\n",
        "        \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.111 Safari/537.36\",\n",
        "        \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/28.0.1500.72 Safari/537.36\",\n",
        "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10) AppleWebKit/600.1.25 (KHTML, like Gecko) Version/8.0 Safari/600.1.25\",\n",
        "        \"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:33.0) Gecko/20100101 Firefox/33.0\",\n",
        "        \"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.111 Safari/537.36\",\n",
        "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.111 Safari/537.36\",\n",
        "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/600.1.17 (KHTML, like Gecko) Version/7.1 Safari/537.85.10\",\n",
        "        \"Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko\",\n",
        "        \"Mozilla/5.0 (Windows NT 6.3; WOW64; rv:33.0) Gecko/20100101 Firefox/33.0\",\n",
        "        \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.104 Safari/537.36\"\n",
        "  ]\n",
        "  return random.choice(uastrings)\n",
        "\n",
        "crawl_res = []\n",
        "\n",
        "# for each page in the top N list we are going to scrape the html content\n",
        "for index, row in gsc_df_clean.iterrows():\n",
        "  try:\n",
        "    USER_AGENT = GET_UA()\n",
        "    headers = {'user-agent': USER_AGENT}\n",
        "    resp = requests.get(row.to_dict()['page'], headers=headers)\n",
        "    # parse the HTML with Beautiful soup\n",
        "    if resp.status_code == 200:\n",
        "        if xpath_selector is not None and xpath_selector != '':\n",
        "            tree = lxml.html.fromstring(resp.content)\n",
        "            # Get element using XPath \n",
        "            selector = f\"string-length({xpath_selector}) - string-length(translate({xpath_selector},' ',''))\"\n",
        "            xpath_selector_count = tree.xpath(selector)\n",
        "            xpath_selector_text = tree.xpath(xpath_selector)\n",
        "            selected_content = b'\\n'.join([lxml.etree.tostring(elem) for elem in xpath_selector_text])\n",
        "            # for elem in xpath_selector_text:\n",
        "            #   print(elem)\n",
        "            crawl_res.append(dict(\n",
        "                page=row.to_dict()['page'],\n",
        "                word_count=int(xpath_selector_count),\n",
        "                source_text=selected_content\n",
        "            ))\n",
        "\n",
        "  # below exceptions stop errors from breaking tool\n",
        "  except ParserError as pe:\n",
        "    print(\"ParserError: Error Message - {0}\".format(pe))\n",
        "    pass\n",
        "\n",
        "  except BaseException as ge:\n",
        "    print(\"Unidentified Error - {0}\".format(ge))\n",
        "    pass\n",
        "\n",
        "\n",
        "# transform the list of dictionaries into a dataframe to be able to work with the exisiting dataframes\n",
        "crawl_df = pandas.DataFrame([c for c in crawl_res])\n",
        "\n",
        "conds = [\n",
        "    crawl_df.word_count.between(thin_words_1,thin_words_2),\n",
        "    crawl_df.word_count.between(okay_words_1,okay_words_2),\n",
        "    crawl_df.word_count.between(strong_words_1,strong_words_2)\n",
        "]\n",
        "\n",
        "words_range = [\n",
        "    f'{thin_words_1} to {thin_words_2}', \n",
        "    f'{okay_words_1} to {okay_words_2}',\n",
        "    f'{strong_words_1}+'\n",
        "]\n",
        "words_value = [\n",
        "    'Thin',\n",
        "    'Okay',\n",
        "    'Strong'\n",
        "]\n",
        "\n",
        "crawl_df['words_range'] = np.select(conds,words_range)\n",
        "crawl_df['words_value'] = np.select(conds,words_value)\n",
        "crawl_df.to_excel(\n",
        "    excel_file,\n",
        "    sheet_name='Crawl',\n",
        "    header=True)\n",
        "data_table.DataTable(crawl_df, include_index=False, num_rows_per_page=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ny0LGIlon55H"
      },
      "source": [
        "#@title Merge & Prepare Data { display-mode: \"form\" }\n",
        "output = gsc_df_clean.merge(crawl_df, how=\"left\")\n",
        "\n",
        "select_cols_final = [\n",
        "    \"gsc_datasetID\",\n",
        "    \"domain\",\n",
        "    \"page\",\n",
        "    \"clicks\",\n",
        "    \"clicks_range\",\n",
        "    \"clicks_value\",\n",
        "    \"word_count\",\n",
        "    \"words_range\",\n",
        "    \"words_value\"\n",
        "]\n",
        "\n",
        "output = output[select_cols_final]\n",
        "\n",
        "conds = [\n",
        "    (output.clicks_value == clicks_value[0]) & (output.words_value == words_value[0]),\n",
        "    (output.clicks_value == clicks_value[1]) & (output.words_value == words_value[0]),\n",
        "    (output.clicks_value == clicks_value[2]) & (output.words_value == words_value[0]),\n",
        "\n",
        "    (output.clicks_value == clicks_value[0]) & (output.words_value == words_value[1]),\n",
        "    (output.clicks_value == clicks_value[1]) & (output.words_value == words_value[1]),\n",
        "    (output.clicks_value == clicks_value[2]) & (output.words_value == words_value[1]),\n",
        "\n",
        "    (output.clicks_value == clicks_value[0]) & (output.words_value == words_value[2]),\n",
        "    (output.clicks_value == clicks_value[1]) & (output.words_value == words_value[2]),\n",
        "    (output.clicks_value == clicks_value[2]) & (output.words_value == words_value[2])\n",
        "]\n",
        "\n",
        "actions = [\n",
        "    'Either redirect or refresh',\n",
        "    'Either redirect or refresh',\n",
        "    'Refresh',\n",
        "\n",
        "    'Either redirect or refresh',\n",
        "    'Either no change, redirect or refresh',\n",
        "    'Either no change or refresh',\n",
        "\n",
        "    'Either no change or refresh',\n",
        "    'No change',\n",
        "    'No change'\n",
        "]\n",
        "\n",
        "merged_value = [\n",
        "    f'{clicks_value[0]}/{words_value[0]}',\n",
        "    f'{clicks_value[1]}/{words_value[0]}',\n",
        "    f'{clicks_value[2]}/{words_value[0]}',\n",
        "    \n",
        "    f'{clicks_value[0]}/{words_value[1]}',\n",
        "    f'{clicks_value[1]}/{words_value[1]}',\n",
        "    f'{clicks_value[2]}/{words_value[1]}',\n",
        "\n",
        "    f'{clicks_value[0]}/{words_value[2]}',\n",
        "    f'{clicks_value[1]}/{words_value[2]}',\n",
        "    f'{clicks_value[2]}/{words_value[2]}',\n",
        "]\n",
        "\n",
        "output['merged_value'] = np.select(conds,merged_value)\n",
        "output['calculated_actions'] = np.select(conds,actions)\n",
        "\n",
        "output.to_excel(\n",
        "    excel_file,\n",
        "    sheet_name='Merge',\n",
        "    header=True)\n",
        "\n",
        "data_table.DataTable(output, include_index=False, num_rows_per_page=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8QV6KflgjU_"
      },
      "source": [
        "with pandas.ExcelWriter(os.path.join(path,\"final-content-audit_{}-{}.xlsx\".format(domain_name,date))) as writer:\n",
        "    output.to_excel(writer, sheet_name='Final Dataset', index=False)\n",
        "    gsc_df.to_excel(writer, sheet_name='GSC Data', index=False)\n",
        "    crawl_df.to_excel(writer, sheet_name='Extracted Content', index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WytrK2FMBs7e"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}